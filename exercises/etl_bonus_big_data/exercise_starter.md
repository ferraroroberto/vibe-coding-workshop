## English

# Exercise: The Big Data Stress Test

## The scenario

Fast forward two years. Your company has grown, and the "Sales Data" is no longer just 3 simple files — it's gigabytes of server logs. Traditional scripts try to load everything into RAM at once and crash. CSV files are slow; text-based logs take up too much space.

## Your mission

Process these simulated large datasets without crashing your computer. Move from memory-heavy tools (like Pandas) to an "Out-of-Core" processing engine that handles data larger than your RAM.

1. Experience why the "old way" fails.
2. Use a new tool (**DuckDB**) to query data *without* loading it all first.
3. Convert bulky CSVs into the efficient **Parquet** format.

## Expected result

You have run queries on large data using DuckDB and exported (or converted) data to Parquet — without running out of memory.

---

## Español

# Ejercicio: La Prueba de Estrés de Big Data

## El escenario

Avancemos dos años. Tu empresa ha crecido y los "Datos de Ventas" ya no son solo 3 archivos simples — son gigabytes de logs del servidor. Los scripts tradicionales intentan cargar todo en RAM de una vez y se cuelgan. Los CSV son lentos; los logs en texto ocupan demasiado espacio.

## Tu misión

Procesar estos conjuntos de datos grandes simulados sin que tu computadora se cuelgue. Pasar de herramientas que consumen mucha memoria (como Pandas) a un motor de procesamiento "Out-of-Core" que maneja datos más grandes que tu RAM.

1. Experimentar por qué el "método antiguo" falla.
2. Usar una nueva herramienta (**DuckDB**) para consultar datos *sin* cargarlos todos primero.
3. Convertir CSVs voluminosos al eficiente formato **Parquet**.

## Resultado esperado

Has ejecutado consultas sobre datos grandes con DuckDB y exportado (o convertido) datos a Parquet — sin quedarte sin memoria.
