## English

# Exercise: The Big Data Stress Test

## The Goal
Fast forward two years. Your company has grown, and the "Sales Data" is no longer just 3 simple files—it's gigabytes of server logs.

Your mission is to process these simulated large datasets without crashing your computer. You will move from memory-heavy tools (like Pandas) to an "Out-of-Core" processing engine that handles data larger than your RAM.

## The Problem
*   **System Crashes:** Traditional scripts try to load everything into RAM at once.
*   **Slow Performance:** CSV files are slow to read and process.
*   **Storage Costs:** Text-based logs take up too much space.

## The Expected Outcome
1.  Experience why the "old way" fails.
2.  Use a new tool (**DuckDB**) to query data *without* loading it all first.
3.  Convert bulky CSVs into the efficient **Parquet** format.

---

## Español

# Ejercicio: La Prueba de Estrés de Big Data

## El Objetivo
Avancemos dos años. Tu empresa ha crecido y los "Datos de Ventas" ya no son solo 3 archivos simples—son gigabytes de logs del servidor.

Tu misión es procesar estos conjuntos de datos grandes simulados sin que tu computadora se cuelgue. Pasarás de herramientas que consumen mucha memoria (como Pandas) a un motor de procesamiento "Out-of-Core" que maneja datos más grandes que tu RAM.

## El Problema
*   **Crashes del Sistema:** Los scripts tradicionales intentan cargar todo en RAM de una vez.
*   **Rendimiento Lento:** Los archivos CSV son lentos para leer y procesar.
*   **Costos de Almacenamiento:** Los logs en texto ocupan demasiado espacio.

## El Resultado Esperado
1.  Experimentar por qué el "método antiguo" falla.
2.  Usar una nueva herramienta (**DuckDB**) para consultar datos *sin* cargarlos todos primero.
3.  Convertir CSVs voluminosos al eficiente formato **Parquet**.
